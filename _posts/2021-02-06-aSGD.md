---
title: 'Paper Summary: Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization'
date: 2021-02-06
permalink: /posts/2021/02/aSGD/
tags:
  - Online Optimization
  - paper summary
---
In this blog, I will give a summary of the paper ["Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization"](https://icml.cc/2012/papers/261.pdf) and some of my understandings. A full version with proofs can be found at [arxiv](https://arxiv.org/pdf/1109.5647.pdf). For stardard stochastic gradient descent (SGD) method with strongly convex problem, the convergence rate is known to be $O(log(T)/T)$, while there are other algorithms achieves $O(1/T)$ convergence rate. Is $O(log(T)/T)$ a tight bound for SGD?
In this paper, the authors give an example of strongly convex and non-smooth problem where the stardard stochastic gradient descent (SGD) has a $\Omega(log(T)/T)$. This justify the claim that $O(log(T)/T)$ is a tight bound for stardard SGD. Moreover, a simple modification of SGD is shown to be sufficient to recover the $O(1/T)$ rate. 

### 1. Stochastic Gradient Descent
Stochastic Gradient Descent (SGD) is a well-known and popular method to solve a convex stochastic optimization problem. The goal is to minimize the objective $F$ over some convex domain $W$, without the knowledge of $F$. The only information we can obtain is a stochastic gradient oracle. For each input $w\in W$, the oracle will output a vector $\hat{g}$ whose expectation $E(\hat{g}) = g \in \partial F(w)$