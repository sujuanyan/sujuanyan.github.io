---
title: 'Paper Summary: Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization'
date: 2021-02-06
permalink: /posts/2021/02/aSGD/
tags:
  - Online Optimization
  - paper summary
---
In this blog, I will give a summary of the paper ["Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization"](https://icml.cc/2012/papers/261.pdf) and some of my understandings. A full version with proofs can be found at [arxiv](https://arxiv.org/pdf/1109.5647.pdf). For stardard stochastic gradient descent (SGD) method with strongly convex problem, the convergence rate is known to be $O(log(T)/T)$, while there are other algorithms achieves $O(1/T)$ convergence rate. Is $O(log(T)/T)$ a tight bound for SGD?
In this paper, the authors give an example of strongly convex and non-smooth problem where the stardard stochastic gradient descent (SGD) has a $\Omega(log(T)/T)$. This justify the claim that $O(log(T)/T)$ is a tight bound for stardard SGD. Moreover, a simple modification of SGD is shown to be sufficient to recover the $O(1/T)$ rate. 

### 1. Stochastic Gradient Descent
Stochastic Gradient Descent (SGD) is a well-known and popular method to solve a convex stochastic optimization problem. The goal is to minimize the objective $F$ over some convex domain $W$, without the knowledge of $F$. The only information we can obtain is a stochastic gradient oracle. For each input $w\in W$, the oracle will output a vector $\hat{g}$ whose expectation $E(\hat{g}) = g \in \partial F(w)$ is a subgradient of $F$ at $w$. 

In SGD method, we first arbitrarily select a initial point $w$ inside the convex domain $W$. Then at each iteration step $t=1,\dots,T$, we call the stochastic gradient oracle to obtain the $\hat{g}_t$, then update the optimization variable by 

$$
w_{t+1} = \Pi_{W}(w_t-\eta_t \hat{g}_t)
$$

Here, the step size $\eta_t$ is designed by users. $\Pi_{W}$ is the projection operator on the convex domain $W$. After getting the sequence of points $w_1,\dots,w_T$, stardard SGD returns the average point 

$$\bar{w}_T = \frac{1}{T}(w_1+\dots+w_T)$$

For $\lambda$-strongly convex function $F$, the authors consider the general step sizes $\eta_t = {c}/({\lambda t})$ and it can be shown that the step size of $\Theta(1/t)$ is necessary to obtain the optimal convergence rate. 

For smooth problem, the last point of SGD converges with $O(1/T)$. Mathemcatically, suppose $F$ is $\lambda$-stronly convex and $\mu$-smooth with repect to the optimal point $w^*$, and suppose $E[|| \hat{g}_t ||^2] \leq G^2$. Then with step size $\eta_t = c/(\lambda t)$ and $c>1/2$, we have:

$$
E[F(W_T)-F(w^*)] \leq \frac{1}{2}max\{ 4,\; \frac{c}{2-1/c} \} \frac{\mu G^2}{\lambda^2 T}
$$

This inequality claims that the expected difference between the objective at last point $F(w_T)$ and optimal value $F(w^*)$ is at most order of $(1/T)$. For the average point $\bar{w}_T$, it also enjoys an $O(1/T)$ rate.

### 2. Non-Smooth Problems

So far, SGD works well with stronly convex and smooth objective functions. But what about non-smooth functions? In the literature, SGD with with averaging is known to have a $O(log(T)/T)$ 