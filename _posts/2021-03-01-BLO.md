---
title: 'Paper Summary: Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization'
date: 2021-03-01
#permalink: /posts/2021/03/BLO/
tags:
  - Online Optimization
  - paper summary
---

In this blog, I will give a summary of the paper ["Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization"](http://web.eecs.umich.edu/~jabernet/123-Abernethy.pdf) and some of my understandings. As its title suggests, this paper proposes an efficient algorithm for bandit linear optimization, which achieves the optimal $O^*(\sqrt{T})$ regret. This method studies the Hessian matrix of regularization function $R(x)$ to estimate the cost function which shows its connection to interior point methods.

### 1. Bandit Linear Optimization

As introduced in the previous blog, the *online linear optimization* problem is defined as the following repeated game between the learner (player) and the environment(adversary).

At each time step t=1 to T
- Player chooses $x_t \in \mathcal{K}$
- Adversary *independently* chooses $f_t\in \mathbb{R}^n$
- Player suffers loss $f_t^T x_t$ and observes feedback $s$.

The Player's goal is to minimize his *regret* $R_T$ defined as 

$$
R_T = \sum_{t=1}^T f_t^Tx_t - \min_{x^*\in\mathcal{K}}\sum_{t=1}^T f_t^T x^*
$$

The above problem has two versions: *full-information* and *bandit*. In full-information setting, the Player may observe the entire function $f_t$ and its feedback $s$, as discussed in the earlier online gradient descent (OGD) blog. This paper considers a more challenging bandit setting, where only the feedback $s=f_t x_t$ is provided to the Player, but not the function $f_t$. At the first glance, the bandit setting is more difficult than the full-information setting bacause of lacking information. But suprisingly, the optimal regret bound is $O^*(\sqrt{T})$ similar to full-information setting. The method proposed in this paper actually achieves this optimal bound. The cost we pay for less information only relates to the dimension the desition variables $n$ but not the number of total rounds $T$.

### 2. Explore and Exploit

To cope with lack of information, one reasonable direction is to construct an unbiased estimation $\tilde{f}_t$ of the cost function $f_t$ and feed the estimates $\tilde{f}_t$ to a full-information algorithm $\mathcal{A}$ lick OGD or follow the regularized leader (FTRL). Formally, we introduce the concepts of explore and exploit:

- **Explore:** Construct some random estimate $\tilde{f}_t$ in such a way that $E[\tilde{f}_t]=f_t$.
- **Exploit:** Query some full-information algorithm $\mathcal{A}$ to get the decision variable $x_t$.

In the literature, there are roughly two categories of approches:

- **Alternating Explore/Exploit:** Flip an $\epsilon$-biased coin to determine whether to explore or exploit. On explore rounds, sample uniformly on some wide region around $\mathcal{K}$ and estimate $f_t$ accordingly, and input this into $\mathcal{A}$. On exploit rounds, query $\mathcal{A}$ for $x_t$ and predict this. These methods fail to obtain the desired $O(\sqrt{T})$ bound since they do not make fully use of the information at each round. In fact, $\Omega(T^{2/3})$ regret is unavoidable for these alorithms.
- **Simultaneous Explore/Exploit:** Query $\mathcal{A}$ for $x_t$ and construct a random vector $X_t$ such that $E[X_t] = x_t$. Construct $\tilde{f}_t$ randomly based on the outcome of $X_t$ and the learned value $f_t^T X_t$. The methods in this category are more sophisticated and motivate the approach in this paper.

### 3. The Curse of High Variance and the Blessing of Regularization

The authors then review two of previous work falling in the second category. They point out that one of the main difficulty in bandit setting comes from the high variance of the estimate $\tilde{f}_t$. That is, the regret bound OGD on $\tilde{f}_t$ involves terms $E[||\tilde{f}_t||^2]=O(1/r^2)$ where $r$ is the distance to the boundry. This means that the regret scales as the inverse of the squared distance to the bountry. If we require $\tilde{f}_t$ be unbiased and $x_t$ be the center of the sampling distribution, the high variance property can be shown to be intrinsic to the problem and thus not avoidable.

Fortunately, if we choose the regularization function $R$ carefully, the algorithm will give a better behavior. The authors call it "the blessing of regularization". Indeed, the formulation of the regularized minimization as a dual-space mirror descent comes to the rescue. The choice of regularization term motivates from the entropy function $R(x) = \sum_{i=1}^n x[i]log(x[i])$. By taking the second derivative, the curvature of the entropy function increases $1/x[i]$. So in this paper, they choose a regularization function $R(x)$ that curves as inverse *squared* distance to the boundary, which is so called *self-concordant barriers* in the area of theory of interior point methods.

A 

### Conclusion

### Some Thoughts

To be honest, this is a quite hard paper to read. There are tons of mathemtical concepts and proofs which are not easy to follow. I can hardly say I fully understand all the insights behind this paper. But this is also a good paper that worth reading multiple times since there are many insights and techniques which we can learn.

### References
