---
title: 'Paper Summary: Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization'
date: 2021-03-01
#permalink: /posts/2021/03/BLO/
tags:
  - Online Optimization
  - paper summary
---

In this blog, I will give a summary of the paper ["Competing in the Dark: An Efficient Algorithm for Bandit Linear Optimization"](http://web.eecs.umich.edu/~jabernet/123-Abernethy.pdf) and some of my understandings. As its title suggests, this paper proposes an efficient algorithm for bandit linear optimization, which achieves the optimal $O^*(\sqrt{T})$ regret. This method studies the Hessian matrix of regularization function $R(x)$ to estimate the cost function which shows its connection to interior point methods.

### 1. Bandit Linear Optimization

As introduced in the previous blog, the *online linear optimization* problem is defined as the following repeated game between the learner (player) and the environment(adversary).

At each time step t=1 to T
- Player chooses $x_t \in \mathcal{K}$
- Adversary *independently* chooses $f_t\in \mathbb{R}^n$
- Player suffers loss $f_t^T x_t$ and observes feedback $s$.

The Player's goal is to minimize his *regret* $R_T$ defined as 

$$
R_T = \sum_{t=1}^T f_t^Tx_t - \min_{x^*\in\mathcal{K}}\sum_{t=1}^T f_t^T x^*
$$

The above problem has two versions: *full-information* and *bandit*. In full-information setting, the Player may observe the entire function $f_t$ and its feedback $s$, as discussed in the earlier online gradient descent (OGD) blog. This paper considers a more challenging bandit setting, where only the feedback $s=f_t x_t$ is provided to the Player, but not the function $f_t$. 


### Conclusion

### Some thoughts

To be honest, this is a quite hard paper to rea. There are tons of mathemtical concepts and proofs which is not easy to follow. I can hardly say I fully understand all the insights behind this paper. But this is also a good paper that worth reading multiple times since there are many insights and techniques which we can learn.

### References
