---
title: 'Paper Summary: Online Convex Programming and Generalized Infinitesimal Gradient Ascent'
date: 2021-01-25
permalink: /posts/2021/01/GIGA/
tags:
  - Online Optimization
  - paper summary
---
In this blog, I will give a summary of the paper ["Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. Proc. of ICML, 2003."](https://www.cs.cmu.edu/~maz/publications/techconvex.pdf) and talk about some of my understandings. To the best of my knowledge, this is the first paper that introduces the *online convex programming*, which is a generalization of the well-studied experts problem. Besides, A simple and natural method is proposed, namely the *generalized Infinitesimal Gradient Ascent (GIGA)*. This algorithm is straightforward but powerful with certain advantages. The paper first introduces the form online convex programming followed by a greedy projection algorithm. Then the GIGA method is proposed in the context of repeated games. The GIGA method is a generalized version of the algorithm *infinitesimal gradient ascent*, which is originally proposed for repeated games.

### 1. Online Convex Programming and Greedy Projection

In the traditional convex optimization problem, we want to find a decision variable inside a constraint set such that it suffers the minimum cost. Every parameters of the problem is given beforehand. However, in an *online convex programming problem*, we do not know the cost function until the decision is made. Mathematically, given a feasible set $F \subseteq \mathbb{R}^{n}$, at each time step $t$, an *onine convex programming algorithm* selects a vector $x^t \in F$, and then receives the cost function $c^t$. Note that the algorithm does not know what the cost function $c^t$ is before it makes the decision. 

Because an online algorithm does not have the full information, it will probably produce a sequence of $\{x^1,x^2,...\}$ that is not optimal. To measure the performance of an online algorithm, we compare it with the fixed optimal solution $x$ generated by an algorithm with the full knowledge of the cost functions. Mathematically, we define the regret of algorithm $A$ until time $T$:

$$
R_A(T) = \sum_{t=1}^T c^t(x^t) - \underset{x \in F}{\min} \sum_{t=1}^T C^t(x)
$$

### 2. 

$$
R_G(T) \leq \frac{\| F \|^2 \sqrt{T}}{2} 
$$


